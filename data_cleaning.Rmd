---
title: "data_cleaning"
author: "Guangze Yu"
date: "2/6/2022"
output: html_document
---

```{r}
library(tidyverse)
library(tidyr)
library(ggplot2)
library(dplyr)
disease <- read.csv("Downloads/Disease Severity and Meaning Making in SLOS_February 1, 2022_16.39.csv", header=TRUE,na.strings=c("","NA"))
disease = disease[-1:-2,]

disease <- disease %>% select(-StartDate,-EndDate,-Status,-IPAddress,-Duration..in.seconds.,-RecordedDate,-ResponseId,-RecipientFirstName,-RecipientEmail,-RecipientLastName,-ExternalReference,-LocationLatitude,-LocationLongitude,-DistributionChannel,-UserLanguage,-Q_RecaptchaScore) %>% mutate(id=row_number())

# Remove rows: process < 10 & severity_score =0
anormal_data <- disease %>% 
  filter(as.numeric(as.character(Progress)) < 10 & as.numeric(as.character(SC0))==0)
# Subset for text-only dataset (Q33,34,35 find the meaning related)
meaning <- disease %>% select(Q33,Q34,Q35)
# The subset after remove anomal rows based on process and severity_score
normal_data <- disease %>% filter(!id %in% anormal_data$id) %>% as.data.frame()

# manually change the all age with unit year. 
normal_data$Q8 = as.character(normal_data$Q8)
normal_data[7, 16] = 2.5
normal_data[72,16] = 0.75
normal_data[70,16] = 3.92
normal_data[64,16] = 3.17
normal_data[8,16] = 0.92
normal_data[74,16] = 1.5
normal_data[54,16] = 26

#Convert severity score to 100-point scale.
#normal_data <- normal_data %>% mutate(SC100 = as.numeric(SC0)/20*100) 
normal_data <- normal_data %>% rowwise() %>% mutate(missing_detectQOL=sum(is.na(c_across(Q28_1:Q31_5))))
#Calculate QOL.
QOL <- function(x){
  x <-  
    ifelse(x==1,100,ifelse(x==2,75,ifelse(x==3,50,ifelse(x==4,25,ifelse(x==5,0," ")))))
}
normal_data[,36:58] <- QOL(normal_data[,36:58])

normal_data[,36:58] <- apply(normal_data[,36:58],2,function(x) as.numeric(x))

normal_data$QOL <- apply(normal_data[,36:58], 1, mean, na.rm = TRUE)
####### whether we should exclude 18 
normal_data <- normal_data %>% filter(as.numeric(Q8) <18)

length(is.null(normal_data$Q28_2))


normal_data <- normal_data %>% rowwise() %>% mutate(missing_detectQOL=sum(is.na(c_across(Q28_1:Q31_5))))

# more than 50% should not be counted 
# if missing_detectQOL > 23/2, this row don't hava life-quality score. 
table(normal_data$missing_detectQOL)
table(normal_data$Q8)

a <- normal_data %>% filter(missing_detectQOL <=10) 

fit1 <- lm(QOL ~ as.numeric(as.character(SC0)), data = a)
summary(fit1)
plot(as.numeric(as.character(a$SC0)),a$QOL,type="p")
abline(fit1)

over_4 <- a %>% filter(as.numeric(as.character(SC0))>=4) %>% select(SC0,QOL,id) %>% mutate(group = c("1"))
less_4 <- a %>% filter(as.numeric(as.character(SC0))<4) %>%
select(SC0,QOL,id) %>% mutate(group = c("2"))
total <- rbind(over_4,less_4)

# t.test(spending ~ city, var.equal = TRUE)

t.test(QOL ~ group, data = total,
        var.equal = TRUE, alternative = "less")

```

```{r}
# text analysis
# Q33 the description of children from parents
library(sentimentr)
library(stringi)
library(tidytext)
library(stringr)
q33_word <- stri_extract_all_regex(a$Q33, "\\w+") 
q33_word <- q33_word[-26] 

b <- a$Q33 %>%unlist() %>% as.data.frame()
names(b)[names(b) == '.'] <- 'text'
b$text <- as.character(b$text)

tidy_data <- b %>% 
  mutate(patient_id = as.character(a$id)) %>% 
  ungroup() %>%
  unnest_tokens(word,text)

most_frequency_word <-tidy_data %>%
 inner_join(get_sentiments("nrc")) %>%
 count(word, sentiment,sort = TRUE)

frequency_sentiment <- tidy_data %>%
  right_join(get_sentiments("nrc")) %>% 
  filter(!is.na(sentiment),!is.na(patient_id)) %>%
  group_by(patient_id)%>%
  arrange(as.numeric(patient_id))

#analyze the relationship between sentiment and Disease severity and QOL
afinn <- tidy_data %>%
  inner_join(get_sentiments("afinn")) %>% 
  group_by(patient_id)%>%
  summarise(sentiment = sum(value)) %>% 
  rename(id=patient_id) %>% 
   mutate(id = as.integer(afinn$id))%>% 
  inner_join(a,by="id")
fit2 <- lm(sentiment ~ QOL, data = afinn)
summary(fit2)
plot(afinn$QOL,afinn$sentiment,type="p")
abline(fit2)
# There seems to be a negative relationship between QOL and sentiment, but not significant

fit3 <- lm(sentiment ~ as.numeric(SC0), data = afinn)
summary(fit3)
plot(as.numeric(afinn$SC0),afinn$sentiment,type="p")
abline(fit3)
#There seems to be a positive relationship between SC0 and sentiment, but not significant

#Apply linear regression model with QOL and SC0 as predictors, and the coefficients are not significant.
fit4 <- lm(sentiment ~ as.numeric(SC0)+ QOL, data = afinn)
summary(fit4)
```


To more specific of sentiment analysis, we try to include a word dictionary which have more than positive and negative sentiment. We choose "NRC". The specific sentiment emotional list reference website: NRC dictionary.
https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm

Eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive)
So, for some description like "strong", although under our background, we assume this is a positive word but when using "nrc" dictionary to analyze. We cannot find out. 
However, based on the frequency of each type of sentiment appear for each patient, we can have an outline of sentiment of this patient. 
The dataframe named 'most_frequency_word' reflects which word appear the most among our 34 patients. 
The dataframe named 'frequency_sentiment' reflects how much sentiment word for each patient. 

We also used the "afinn" lexicon, which put positive or negative values to each word. We regressed sentiment score on QOL and severity score, and we couldn't find any relationship between them. 

```{r}
# Q34 text analysis
q_34 <- a %>% select(Q34,id)
q_34$Q34 <- as.character(q_34$Q34)
tidy_34 <- q_34%>%
  ungroup() %>%
  unnest_tokens(word, Q34)

sentiment_score_34 <- tidy_34 %>%
  inner_join(get_sentiments("afinn")) %>%
  group_by(id) %>%
  summarise(sentiment = sum(value, na.rm = TRUE))
```


To get the sentiment score of each patient, we use another dictionary called "afinn", which will give a score to each word. The specific link for this dictionary is 'https://github.com/fnielsen/afinn/tree/master/afinn/data'. Then, we try to sum up each patient's score. More positive large means more positive sentiment. 

However, there is only 25 patients who have sentiment score. So, it is useless to plot the relationship between sentiment score and severity score. The limitation of this analysis is that there is not a "perfect" dictionary to match each word. All in all, the trend is reliable. So, we can conclude that most patients have positive sentiment. 


```{r}
#q32
q_32 <- a %>% select(Q32,Q32_4_TEXT,id)
q_32$Q32 <- as.character(q_32$Q32)
q_32$Q32_4_TEXT <- as.character(q_32$Q32_4_TEXT)

c <- separate_rows(q_32, Q32, sep = ",")
table(c$Q32)
# the frequency of choices appeared.
```

Don't know to analyze Q32.


```{r}
# Q35 text analysis
q_35 <- a %>% select(Q35,id)
q_35$Q35 <- as.character(q_35$Q35)
tidy_35 <- q_35%>%
  ungroup() %>%
  unnest_tokens(word, Q35)

sentiment_score_35 <- tidy_35 %>%
  inner_join(get_sentiments("afinn")) %>%
  group_by(id) %>%
  summarise(sentiment = sum(value, na.rm = TRUE))
```
Same procedure for Q35 and Q34.





